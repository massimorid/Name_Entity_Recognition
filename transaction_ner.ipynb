{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U89uBCNa0bdQ"
      },
      "source": [
        "###Setup\n",
        "1. Importing Libraries\n",
        "2. Mounting Drive\n",
        "3. Defining `BASE_PATH` & `TARGET_COUNTRY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hpQTZlfq_1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f68915-0b3c-4bc1-d229-010c360ac24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair==0.15.1\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair==0.15.1)\n",
            "  Downloading boto3-1.41.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair==0.15.1)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair==0.15.1)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair==0.15.1)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (0.36.0)\n",
            "Collecting langdetect>=1.0.9 (from flair==0.15.1)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (6.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (10.8.0)\n",
            "Collecting mpld3>=0.3 (from flair==0.15.1)\n",
            "  Downloading mpld3-0.5.12-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pptree>=3.1 (from flair==0.15.1)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (2.9.0.post0)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair==0.15.1)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (2025.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (1.6.1)\n",
            "Collecting segtok>=1.5.11 (from flair==0.15.1)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair==0.15.1)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.12/dist-packages (from flair==0.15.1) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair==0.15.1)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair==0.15.1) (4.57.2)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair==0.15.1)\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair==0.15.1)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair==0.15.1)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair==0.15.1)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair==0.15.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.42.0,>=1.41.5 (from boto3>=1.20.27->flair==0.15.1)\n",
            "  Downloading botocore-1.41.5-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair==0.15.1)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.16.0,>=0.15.0 (from boto3>=1.20.27->flair==0.15.1)\n",
            "  Downloading s3transfer-0.15.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2.13->flair==0.15.1) (2.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1.0->flair==0.15.1) (0.2.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair==0.15.1) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair==0.15.1) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair==0.15.1) (2.32.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair==0.15.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair==0.15.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair==0.15.1) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair==0.15.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->flair==0.15.1) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect>=1.0.9->flair==0.15.1) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->flair==0.15.1) (3.2.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mpld3>=0.3->flair==0.15.1) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair==0.15.1) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair==0.15.1) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair==0.15.1) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->flair==0.15.1) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair==0.15.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair==0.15.1) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair==0.15.1) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair==0.15.1) (5.29.5)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.5->boto3>=1.20.27->flair==0.15.1) (2.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair==0.15.1) (25.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.1->flair==0.15.1) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair==0.15.1) (1.12.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair==0.15.1) (2.8)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair==0.15.1) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mpld3>=0.3->flair==0.15.1) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair==0.15.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair==0.15.1) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair==0.15.1) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair==0.15.1) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair==0.15.1) (5.9.5)\n",
            "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.41.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.12-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading transformer_smaller_training_vocab-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.41.5-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.15.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=b256f82663f5f8bab2e2781253fa2cb589065fc4bdcdbcce65a9b1002f10712a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=69cdb11c0a71e3977c8a4f971332d475bc567b2d81d2ebe6da581155ed512adf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/2d/de/37058114a8f07cfec75747cb46b864bc5c71b0e9e0e4cd0acd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=36711a2e2cea20a670430314d9b52f1ca0471f6b6c9521b94c4db07c55331491\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=0bd6e938cf4fd8f3d24e0e3f8961e5def14caef656f530fb1a09d58114077f2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=843befa9fee91c12b9c5af2d03901c0f20e037588aa55b34a05b583be078d020\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=0c044a478db6825c17f433d31124670209e93f2d857b9e4f1998af2e551c4827\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, langdetect, jsonlines, jmespath, intervaltree, ftfy, deprecated, conllu, wikipedia-api, botocore, bioc, s3transfer, mpld3, pytorch-revgrad, boto3, transformer-smaller-training-vocab, flair\n",
            "Successfully installed bioc-2.1 boto3-1.41.5 botocore-1.41.5 conllu-4.5.3 deprecated-1.3.1 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.12 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.15.0 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.2 wikipedia-api-0.8.1\n"
          ]
        }
      ],
      "source": [
        "# IF RUNNING THE NEW MODEL\n",
        "!pip install flair==0.15.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IF RUNNING THE OLD MODEL\n",
        "#!pip install flair==0.12.2"
      ],
      "metadata": {
        "id": "BiF4AzHnVHBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGQa_ljJ0sMe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "from nltk.tokenize import word_tokenize\n",
        "from random import choice\n",
        "import re\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t52yVnvy080Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "fdc1fce7-8a03-48ed-a379-64b7ac44edae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3329394316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c21W0OCI0-95"
      },
      "outputs": [],
      "source": [
        "TARGET_COUNTRY = 'be'\n",
        "BASE_PATH = Path('/content/drive/MyDrive/deep-learning/transactions/')\n",
        "DATASET_PATH = BASE_PATH / 'Datasets-Versions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJC5jbFN1Y1M"
      },
      "source": [
        "###DF Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdbbzRnKJaz-"
      },
      "source": [
        "####Building cleaned_text_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdZt_6YHDhwX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(BASE_PATH/'datasets'/'df_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgrf4tcQD2Ej"
      },
      "outputs": [],
      "source": [
        "#filter to country\n",
        "df = df[df['country'] == TARGET_COUNTRY]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS5Xa82oEE8R"
      },
      "outputs": [],
      "source": [
        "#organize df\n",
        "df = df[['country',\t'language',\t'VATType',\t'baseCurrencyAmount',\t'category_id',\t'APC1',\t'APC2',\t'counterparty_name',\t'communication']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cD4AYxs9Bnh"
      },
      "outputs": [],
      "source": [
        "iban_re = re.compile(\n",
        "    r'\\b'                                            # Anchor: Start at a word boundary\n",
        "    r'(?:'                                           # Start non-capturing group for the two formats\n",
        "      r'[A-Z]{2}[0-9]{2}'                            # Mandatory start: Country Code (AA) + Check Digits (##)\n",
        "      r'(?:[\\s]?[A-Z0-9]{4}){2,7}'                   # 2 to 7 groups of (optional space + 4 alphanumeric chars)\n",
        "      r'(?:[\\s]?[A-Z0-9]{1,4})?'                     # Optional final group of 1 to 4 chars, optionally preceded by a space\n",
        "    r'|'                                             # OR\n",
        "      r'[A-Z]{2}[0-9]{2}[A-Z0-9]{10,30}'             # AA## followed by 10 to 30 contiguous alphanumeric chars\n",
        "    r')'\n",
        "    r'\\b',                                           # Anchor: End at a word boundary\n",
        "    flags=re.IGNORECASE                              # Match case-insensitively (most IBAN parsers ignore case on letters)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpO2PUzJ9E7c"
      },
      "outputs": [],
      "source": [
        "big_num_re = re.compile(r'\\d{5,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOn_bFWQ9Gxp"
      },
      "outputs": [],
      "source": [
        "dates_re = re.compile(\n",
        "    r'\\d{4}-\\d{2}-\\d{2}'   # yyyy-mm-dd\n",
        "    r'|'\n",
        "    r'\\d{4}/\\d{2}/\\d{2}'   # yyyy/mm/dd\n",
        "    r'|'\n",
        "    r'\\d{4}\\.\\d{2}\\.\\d{2}' # yyyy.mm.dd\n",
        "    r'|'\n",
        "    r'\\d{2}-\\d{2}-\\d{4}'   # dd-mm-yyyy\n",
        "    r'|'\n",
        "    r'\\d{2}/\\d{2}/\\d{4}'   # dd/mm/yyyy\n",
        "    r'|'\n",
        "    r'\\d{2}\\.\\d{2}\\.\\d{4}' # dd.mm.yyyy\n",
        "    r'|'\n",
        "    r'\\b\\d{2}[-/\\.]\\d{2}[-/\\.]\\d{2}\\b'      # 02-10-23, 10/02/23, etc.\n",
        "    r'|'\n",
        "    r'\\b\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*[\\s,]+\\d{4}\\b'   # 02 Oct 2023, 2 October 2023\n",
        "    r'|'\n",
        "    r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4}\\b'    # Oct 02, 2023, October 2 2023\n",
        "    r'|'\n",
        "    r'\\b\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}(:\\d{2})?(\\.\\d+)?(Z|[+-]\\d{2}:?\\d{2})?\\b'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAFCu6Rn9Jtd"
      },
      "outputs": [],
      "source": [
        "url_re = re.compile(\n",
        "    r'(?:https?://|ftp://|ftps://|sftp://|file://|mailto:|tel:|sms:)?(?:www\\.|m\\.)?',\n",
        "    re.IGNORECASE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS-8ZDsz9LMk"
      },
      "outputs": [],
      "source": [
        "fin_re = re.compile(\n",
        "    r'\\b(?:'\n",
        "    r'sumup'\n",
        "    r'|pluxee'\n",
        "    r'|paypal(?:\\s+europe)?(?:\\s+s\\.?a\\.?r\\.?l\\.?)?(?:\\s+et\\s+cie)?(?:\\s+s\\.?c\\.?a\\.?)?'  # PayPal legal variants\n",
        "    r'|ing(?:\\s+app)?'\n",
        "    r'|kbc(?:\\s+mobile|\\s+brussels|\\s+betaalknop)?'\n",
        "    r'|crelan'\n",
        "    r'|kredbebb\\w*'\n",
        "    r'|bancontact'\n",
        "    r'|maestro'\n",
        "    r'|debit\\s+mastercard'\n",
        "    r'|mastercard'\n",
        "    r'|visa'\n",
        "    r'|apple\\s*pay'\n",
        "    r'|\\bpaypal\\b[\\s,;\\-]*'\n",
        "    r'|paddlenet\\s*[\\n,;\\-]*\\s*(.+)'\n",
        "    r'|google\\s*g\\s*suite[\\s_+\\-]*([\\w\\s]+)'\n",
        "    r')\\b',\n",
        "    flags=re.IGNORECASE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1cok_269M4-"
      },
      "outputs": [],
      "source": [
        "punct_re = re.compile(r'[^\\w\\s\\&]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQZtGHeJ9OYZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(communication, counterparty_name) -> str:\n",
        "    \"\"\"\n",
        "    Combines communication + counterparty_name, fixes encoding,\n",
        "    removes big numbers/dates, cleans text for BERT.\n",
        "    \"\"\"\n",
        "\n",
        "    comm = '' if pd.isna(communication) else str(communication)\n",
        "    cp   = '' if pd.isna(counterparty_name) else str(counterparty_name)\n",
        "\n",
        "    combined = (cp + ' ' + comm).strip()\n",
        "    combined = fix_encoding(combined)\n",
        "    combined = combined.lower()\n",
        "    combined = fin_re.sub('', combined)\n",
        "    combined = iban_re.sub('', combined)\n",
        "    combined = big_num_re.sub('', combined)\n",
        "    combined = dates_re.sub('', combined)\n",
        "    combined = url_re.sub('', combined)\n",
        "    combined = punct_re.sub(' ', combined)\n",
        "    combined = re.sub(r'\\s+', ' ', combined).strip()\n",
        "\n",
        "    return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcpyWIullX1D"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text_bert'] = df.apply(lambda x: preprocess_text(x['communication'], x['counterparty_name']), axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szm8Zw0WIMn4"
      },
      "source": [
        "####Truncate long cleaned_text_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AntEqYCIhj3"
      },
      "outputs": [],
      "source": [
        "#truncate to max 200 length inputs\n",
        "df = df[df['cleaned_text_bert'].str.len() < 200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EWY5LtGGtYe"
      },
      "outputs": [],
      "source": [
        "#drop unkown and Nan in cleaned_text_bert\n",
        "df = df[~(df['counterparty_name'].isna() & df['communication'].isna())]\n",
        "df = df[~(df['cleaned_text_bert'] == '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnJl4o0yCKuZ"
      },
      "outputs": [],
      "source": [
        "#checkpoint\n",
        "df.to_csv(DATASET_PATH/f'df_{TARGET_COUNTRY}_parsed_no_labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV0DjZJpJzxi"
      },
      "source": [
        "####Run Old model for Baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence"
      ],
      "metadata": {
        "id": "dLw59tvaWrSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2FRkAuQX1IS"
      },
      "outputs": [],
      "source": [
        "#loading model\n",
        "model_ner = SequenceTagger.load(BASE_PATH/'old-models'/f'flair_ner_{TARGET_COUNTRY}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtQ-07Jw3AAn"
      },
      "outputs": [],
      "source": [
        "#running predictions\n",
        "sentences = [Sentence(str(text)) for text in df['cleaned_text_bert'].to_list()]\n",
        "model_ner.predict(sentences, mini_batch_size=4096, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F6niapjKt3H"
      },
      "outputs": [],
      "source": [
        "#adding predictions to df\n",
        "df['old_model_results'] = [sentence.get_labels('ner')[0].data_point.text if len(sentence.get_labels('ner')) > 0 else '' for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvSMGAGp2YDl"
      },
      "outputs": [],
      "source": [
        "#checkpoint\n",
        "df.to_csv(BASE_PATH/'Datasets-Versions'/f'df_{TARGET_COUNTRY}_parsed_old_labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then generated the labels for the NER DataFrame with GPT:\n",
        "- `df_be_gpt4o_mini_ner.csv` & `df_be_gpt4o_mini_ner2.csv` (For **BELGIUM**)\n",
        "- `df_de_gpt4o_mini_ner.csv` (For **GERMANY**)\n",
        "\n",
        "Belgium has 2 files as the DataFrame was properly massive"
      ],
      "metadata": {
        "id": "IhDQHTjt2nSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Jaro-Winkler approach"
      ],
      "metadata": {
        "id": "WHLt2VNb34AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(BASE_PATH/f'Datasets-Versions/df_{TARGET_COUNTRY}_gpt4o_mini_ner.csv')\n",
        "df2 = pd.read_csv(BASE_PATH/f'Datasets-Versions/df_{TARGET_COUNTRY}_gpt4o_mini_ner2.csv')"
      ],
      "metadata": {
        "id": "oKAe4NSsA-dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2])"
      ],
      "metadata": {
        "id": "EI8fINjw48oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['extracted_organization'].notna()]"
      ],
      "metadata": {
        "id": "GEtfE1W65nGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset=['hash', 'cleaned_text_bert', 'APC1', 'APC2', 'VATType',\t'baseCurrencyAmount',\t'category_id'], keep='first', inplace=True)"
      ],
      "metadata": {
        "id": "V-PgViO6C_ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2GPL_cXGe1n"
      },
      "outputs": [],
      "source": [
        "!pip install jarowinkler\n",
        "from jarowinkler import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC7rQMLrEcvG",
        "outputId": "364ab816-8d3c-43a8-ed17-3d499eeb834d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(741958, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['old_model_results'], inplace=True)"
      ],
      "metadata": {
        "id": "-oyE1EHlOjxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viyx7_QtUdKY"
      },
      "outputs": [],
      "source": [
        "def old_model_pred(text):\n",
        "    matches = re.findall(r'Span\\[\\d+:\\d+\\]:\\s*[\"\\']([^\"\\']+)[\"\\']', text)\n",
        "    return matches[0] if matches else None\n",
        "\n",
        "df['old_model_results'] = df['old_model_results'].apply(old_model_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwpRI4fWOGA"
      },
      "outputs": [],
      "source": [
        "df.rename(columns = {'extracted_organization':'gpt_generated_labels'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGFWpTTeZE7j"
      },
      "outputs": [],
      "source": [
        "df['gpt_generated_labels'] = df['gpt_generated_labels'].astype(str).apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuF63qVcXAfH"
      },
      "outputs": [],
      "source": [
        "#compute jaro similarity between gpt_labels & old_model_predictions\n",
        "df['jaro_score'] = df.apply(lambda row: jarowinkler_similarity(row['old_model_results'], row['gpt_generated_labels']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucYAABcIXGYP"
      },
      "outputs": [],
      "source": [
        "#filter df to similarity between 0.8-1\n",
        "df = df[df['jaro_score']>=0.8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWeRNgVdPdzS",
        "outputId": "d56aa4a3-b98f-4287-95ff-d0e674cd345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401994, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHy4qhlmUzqV"
      },
      "source": [
        "####Pos Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZgwPyfI96Jf"
      },
      "outputs": [],
      "source": [
        "def pos_tagging(row):\n",
        "    text = str(row['cleaned_text_bert']).lower().split()\n",
        "    label = str(row['gpt_generated_labels']).strip().lower()\n",
        "\n",
        "    if not label:\n",
        "        return ' '.join(['O'] * len(text))\n",
        "\n",
        "    label_words = label.split()\n",
        "    pos = ['O'] * len(text)\n",
        "    label_len = len(label_words)\n",
        "\n",
        "    for i in range(len(text) - label_len + 1):\n",
        "        if text[i:i + label_len] == label_words:\n",
        "            pos[i] = 'B-ORG'\n",
        "            for j in range(1, label_len):\n",
        "                pos[i + j] = 'I-ORG'\n",
        "\n",
        "    return ','.join(pos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJPlsEjJ5aEM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "df['TAG'] = df.progress_apply(pos_tagging, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McMbozjkU9qX"
      },
      "outputs": [],
      "source": [
        "df = df[~ (df['gpt_generated_labels'] == \"unknown\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "892nNczqUONk"
      },
      "outputs": [],
      "source": [
        "def ner_tags_id(x):\n",
        "  words_tags = []\n",
        "  words = x.split(',')\n",
        "  for w in words:\n",
        "    if w == 'O':\n",
        "      words_tags.append(0)\n",
        "    if w == 'B-ORG':\n",
        "      words_tags.append(1)\n",
        "    if w == 'I-ORG':\n",
        "      words_tags.append(2)\n",
        "  return words_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OllaLAnBUDN1"
      },
      "outputs": [],
      "source": [
        "df['ner_tags'] = df['TAG'].apply(ner_tags_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM3hnyJObsWT"
      },
      "outputs": [],
      "source": [
        "df[\"tokens\"] = df[\"cleaned_text_bert\"].astype(str).apply(lambda x: x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JQWOPi7Z3xX"
      },
      "outputs": [],
      "source": [
        "df.to_csv(f\"/content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/{TARGET_COUNTRY}_parsed_ner.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6a3fe90",
        "outputId": "97b15795-385a-4a6f-e31f-f43e4a4c68c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: gensim, bpemb\n",
            "Successfully installed bpemb-0.3.6 gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flair[word-embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCuDCxJtkBG3"
      },
      "outputs": [],
      "source": [
        "from flair.trainers import ModelTrainer\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
        "from flair.models import SequenceTagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2n_sHuBZ5Oy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATASET_PATH / f'{TARGET_COUNTRY}_parsed_ner.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxq7p0V-K_6e"
      },
      "outputs": [],
      "source": [
        "df = df[['cleaned_text_bert', 'gpt_generated_labels']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87d4d280"
      },
      "outputs": [],
      "source": [
        "# Filter to labels in cleaned_text_bert for each row\n",
        "df = df[df.apply(lambda row: str(row['cleaned_text_bert']).lower().find(str(row['gpt_generated_labels']).lower()) != -1, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAQ8obk_Q1nj",
        "outputId": "eff1d02f-37aa-4915-d4ef-9388393df9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(377259, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxOj1crHKiqR"
      },
      "outputs": [],
      "source": [
        "def format_annotation(x):\n",
        "    try:\n",
        "        return [(str(x).lower().strip(), 'ORG')]\n",
        "    except:\n",
        "        print(x)\n",
        "        raise Exception('error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFNXVuzWKhiP"
      },
      "outputs": [],
      "source": [
        "df['gpt_generated_labels'] = df.gpt_generated_labels.apply(format_annotation)\n",
        "df.columns = ['text', 'annotation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9BDVBzgKf4j"
      },
      "outputs": [],
      "source": [
        "df['text'] = df.text.apply(lambda x: str(x).lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxdc7uY-Mv1i"
      },
      "outputs": [],
      "source": [
        "df.loc[df.text.isnull()]\n",
        "df = df.loc[df.text != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikBRHLRvLcRC"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def matcher(string, pattern):\n",
        "    '''\n",
        "    Return the start and end index of any pattern present in the text.\n",
        "    '''\n",
        "    match_list = []\n",
        "    pattern = pattern.strip()\n",
        "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
        "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
        "    if (match.size == len(pattern)):\n",
        "        start = match.a\n",
        "        end = match.a + match.size\n",
        "        match_tup = (start, end)\n",
        "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
        "        match_list.append(match_tup)\n",
        "\n",
        "    return match_list, string\n",
        "\n",
        "def mark_sentence(s, match_list):\n",
        "    '''\n",
        "    Marks all the entities in the sentence as per the BIO scheme.\n",
        "    '''\n",
        "    word_dict = {}\n",
        "    for word in s.split():\n",
        "        word_dict[word] = 'O'\n",
        "\n",
        "    for start, end, e_type in match_list:\n",
        "        temp_str = s[start:end]\n",
        "        tmp_list = temp_str.split()\n",
        "        if len(tmp_list) > 1:\n",
        "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
        "            for w in tmp_list[1:]:\n",
        "                word_dict[w] = 'I-' + e_type\n",
        "        else:\n",
        "            word_dict[temp_str] = 'B-' + e_type\n",
        "    return word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-PFN87vLbyK"
      },
      "outputs": [],
      "source": [
        "def create_data(df, filepath):\n",
        "  '''\n",
        "  The function responsible for the creation of data in the flair required format.\n",
        "  '''\n",
        "  with open(filepath , 'w') as f:\n",
        "    for text, annotation in zip(df.text, df.annotation):\n",
        "      text_ = text\n",
        "      match_list = []\n",
        "      for i in annotation:\n",
        "        a, text_ = matcher(text, i[0])\n",
        "        try:\n",
        "          match_list.append((a[0][0], a[0][1], i[1]))\n",
        "        except Exception as e:\n",
        "          print(a)\n",
        "          print(text_)\n",
        "          raise Exception(e)\n",
        "\n",
        "      d = mark_sentence(text, match_list)\n",
        "\n",
        "      for i in d.keys():\n",
        "        f.writelines(i + ' ' + d[i] +'\\n')\n",
        "      f.writelines('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmYwMB00Q5N3",
        "outputId": "ad9f7108-1925-4c71-8b6b-7dbedf389196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: /content/flair_ner_be: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cd /content/flair_ner_be && ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuHQvvhAm6VV"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, shuffle=True)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMSSGIlNL6me"
      },
      "outputs": [],
      "source": [
        "data_folder = Path(f'/content/flair_ner_{TARGET_COUNTRY}')\n",
        "data_folder.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDSljcvkLsuS"
      },
      "outputs": [],
      "source": [
        "create_data(train_df, data_folder / 'train.txt')\n",
        "create_data(test_df, data_folder / 'test.txt')\n",
        "create_data(val_df, data_folder / 'dev.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhQm0mANuuR"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nxE_XiReLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b6601b-734a-4dd0-ed41-151e1935dae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:56:45,076 Reading data from /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be\n",
            "2025-11-17 00:56:45,077 Train: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/train.txt\n",
            "2025-11-17 00:56:45,077 Dev: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/dev.txt\n",
            "2025-11-17 00:56:45,077 Test: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/test.txt\n",
            "2025-11-17 00:57:24,981 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "264081it [00:05, 46010.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:57:30,734 Dictionary created for label 'ner' with 1 values: ORG (seen 266425 times)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from flair.datasets import ColumnCorpus\n",
        "data_folder = Path('/content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be')  # Modifica con il tuo percorso\n",
        "\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "corpus = ColumnCorpus(\n",
        "    data_folder,\n",
        "    columns,\n",
        "    train_file='train.txt',\n",
        "    test_file='test.txt',\n",
        "    dev_file='dev.txt'\n",
        ")\n",
        "tag_dictionary = corpus.make_label_dictionary(label_type='ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3u9Vil9kItd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb50a971-0f97-49cc-92b4-9cbc3a6d56c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:58:22,753 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpguiiluka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 153M/153M [00:10<00:00, 15.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:58:33,657 copying /tmp/tmpguiiluka to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n",
            "2025-11-17 00:58:33,736 removing temp file /tmp/tmpguiiluka\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:58:34,292 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmpcc013x2g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:02<00:00, 9.59MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:58:37,066 copying /tmp/tmpcc013x2g to cache at /root/.flair/embeddings/glove.gensim\n",
            "2025-11-17 00:58:37,078 removing temp file /tmp/tmpcc013x2g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 00:58:40,321 SequenceTagger predicts: Dictionary with 5 tags: O, S-ORG, B-ORG, E-ORG, I-ORG\n"
          ]
        }
      ],
      "source": [
        "embedding_types = [WordEmbeddings('glove'),]\n",
        "embeddings : StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "tagger : SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings,tag_dictionary=tag_dictionary,tag_type='ner',use_crf=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IEZcAMXj8uI"
      },
      "outputs": [],
      "source": [
        "trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "trainer.train(f'flair_ner_{TARGET_COUNTRY}', learning_rate=0.1, mini_batch_size=32, max_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "local_model_path = f'flair_ner_{TARGET_COUNTRY}/best-model.pt'\n",
        "drive_save_path = f'{BASE_PATH}/models/flair_ner_{TARGET_COUNTRY}_best_model.pt'\n",
        "\n",
        "# Copy the model to Drive\n",
        "shutil.copy(local_model_path, drive_save_path)\n",
        "\n",
        "print(f\"Model saved to: {drive_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAXlrzqjXAN7",
        "outputId": "e4cf0fa3-03f1-476e-cd27-252a2d9deac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/deep-learning/transactions/models/flair_ner_be_best_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Qb-TA2NIqZ"
      },
      "source": [
        "###Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "from flair.datasets import ColumnCorpus"
      ],
      "metadata": {
        "id": "8fv6MxQlUATA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLPka6SDOD1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff640e14-469d-433c-8d89-b063f49b9483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 10:04:54,997 SequenceTagger predicts: Dictionary with 7 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, <START>, <STOP>\n"
          ]
        }
      ],
      "source": [
        "#Running new model\n",
        "model_path = f'{BASE_PATH}/models/flair_ner_be_best_model.pt'\n",
        "model = SequenceTagger.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = Path(BASE_PATH/'Datasets-Versions/flair_ner_be')\n",
        "\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "corpus = ColumnCorpus(\n",
        "    data_folder,\n",
        "    columns,\n",
        "    train_file='train.txt',\n",
        "    test_file='test.txt',\n",
        "    dev_file='dev.txt'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecJZuqFd3vKc",
        "outputId": "e6ebcfe6-84ae-4a9f-ae6f-eab275c6aabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 10:54:29,668 Reading data from /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be\n",
            "2025-11-17 10:54:29,669 Train: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/train.txt\n",
            "2025-11-17 10:54:29,669 Dev: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/dev.txt\n",
            "2025-11-17 10:54:29,670 Test: /content/drive/MyDrive/deep-learning/transactions/Datasets-Versions/flair_ner_be/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "result = model.evaluate(corpus.dev, gold_label_type='ner')\n",
        "print(f\"\\n{result.detailed_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz3q7ACWx3W9",
        "outputId": "a14b17bf-caed-4adf-d859-5109f50caf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1769/1769 [01:22<00:00, 21.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Results:\n",
            "- F-score (micro) 0.94\n",
            "- F-score (macro) 0.94\n",
            "- Accuracy 0.8868\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ORG     0.9407    0.9394    0.9400     57078\n",
            "\n",
            "   micro avg     0.9407    0.9394    0.9400     57078\n",
            "   macro avg     0.9407    0.9394    0.9400     57078\n",
            "weighted avg     0.9407    0.9394    0.9400     57078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from flair.models import SequenceTagger\n",
        "original_load = torch.load\n",
        "\n",
        "def patched_load(*args, **kwargs):\n",
        "    kwargs['weights_only'] = False\n",
        "    return original_load(*args, **kwargs)\n",
        "torch.load = patched_load\n",
        "\n",
        "\n",
        "model_ner = SequenceTagger.load(BASE_PATH/'old-models'/f'flair_ner_{TARGET_COUNTRY}.pt')\n",
        "torch.load = original_load\n",
        "\n",
        "from flair.trainers import ModelTrainer\n",
        "result = model_ner.evaluate(corpus.dev, gold_label_type='ner')\n",
        "print(f\"\\n{result.detailed_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rODsMVDGc80Z",
        "outputId": "385161ec-c4fe-42a2-c8ed-d49aa3cbdbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 10:57:22,227 SequenceTagger predicts: Dictionary with 7 tags: O, S-ORG, B-ORG, E-ORG, I-ORG, <START>, <STOP>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1769/1769 [01:23<00:00, 21.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-17 10:58:46,585 Evaluating as a multi-label problem: False\n",
            "\n",
            "\n",
            "Results:\n",
            "- F-score (micro) 0.6498\n",
            "- F-score (macro) 0.6498\n",
            "- Accuracy 0.4813\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ORG     0.6632    0.6370    0.6498     57078\n",
            "\n",
            "   micro avg     0.6632    0.6370    0.6498     57078\n",
            "   macro avg     0.6632    0.6370    0.6498     57078\n",
            "weighted avg     0.6632    0.6370    0.6498     57078\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OwQnILSjJL1O",
        "m8Qb-TA2NIqZ"
      ],
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}